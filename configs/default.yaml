# configs/default.yaml
# FinFlow-RL 리팩토링된 기본 설정

seed: 42
device: auto # cpu|cuda|auto

# 데이터 설정
data:
  symbols: [
      "AAPL", # Apple
      "AMGN", # Amgen
      "AXP", # American Express
      "BA", # Boeing
      "CAT", # Caterpillar
      "CRM", # Salesforce
      "CSCO", # Cisco Systems
      "CVX", # Chevron
      "DIS", # Disney
      "DOW", # Dow Inc.
      "GS", # Goldman Sachs
      "HD", # Home Depot
      "HON", # Honeywell
      "IBM", # IBM
      "INTC", # Intel
      "JNJ", # Johnson & Johnson
      "JPM", # JPMorgan Chase
      "KO", # Coca-Cola
      "MCD", # McDonald's
      "MMM", # 3M
      "MRK", # Merck
      "MSFT", # Microsoft
      "NKE", # Nike
      "PG", # Procter & Gamble
      "TRV", # Travelers
      "UNH", # UnitedHealth
      "V", # Visa
      "VZ", # Verizon
      "WBA", # Walgreens Boots Alliance
      "WMT", # Walmart
    ] # 다우존스 30 종목

  # 날짜 기반 데이터 설정
  start: "2008-01-01" # 전체 데이터 시작
  end: "2020-12-31" # train+val 종료
  test_start: "2021-01-01" # 테스트 시작
  test_end: "2024-12-31" # 테스트 종료
  val_ratio: 0.2 # train에서 val 분리 비율
  interval: "1d"
  cache: true

# 환경 설정
env:
  initial_balance: 1000000
  transaction_cost: 0.001
  slippage: 0.0005
  no_trade_band: 0.01  # 0.002 → 0.01로 완화 (1%)
  max_leverage: 1.0
  max_turnover: 0.5
  window_size: 20
  use_advanced_reward: false # PortfolioObjective 사용 여부

# 특성 추출
feature_dim: 12

# 오프라인 학습
offline:
  method: "iql" # 'iql' or 'td3bc'
  epochs: 50
  batch_size: 256

  # IQL 전용
  expectile: 0.7
  temperature: 1.0  # 3.0 → 1.0 (더 sharp한 policy)

  # TD3+BC 전용
  bc_weight: 2.5
  policy_delay: 2
  normalize_q: true

# B-Cell (REDQ/TQC)
bcell:
  # 알고리즘 선택: 'REDQ' 또는 'TQC'
  algorithm: "REDQ" # 기본값은 REDQ

  # 네트워크
  hidden_dims: [256, 256]

  # REDQ 설정 (algorithm: 'REDQ'일 때)
  n_critics: 10 # 원논문 권장: N=10 (기존 5에서 증가)
  m_sample: 2
  utd_ratio: 10  # 20 → 10 (안정성 우선)

  # TQC 설정 (algorithm: 'TQC'일 때)
  # n_critics: 2  # TQC는 적은 수의 critics 사용
  n_quantiles: 25 # 분위수 개수
  top_quantiles_to_drop_per_net: 2 # 상위 분위수 제거 개수
  quantile_embedding_dim: 64 # 분위수 임베딩 차원

  # 학습
  actor_lr: 3e-4
  critic_lr: 3e-4
  alpha_lr: 3e-4
  batch_size: 256 # 학습 배치 크기

  # RL
  gamma: 0.99
  tau: 0.005
  alpha: 0.1  # 0.2 → 0.1 (entropy 감소)

  # 위기 대응
  crisis_threshold: 0.7 # 위기 수준 임계값

  # 버퍼
  buffer_size: 100000

  # Dirichlet 정책 설정 (포트폴리오 가중치 생성)
  dirichlet:
    concentration_init: 1.0 # 초기 concentration 매개변수 (낮을수록 더 집중된 분포)
    concentration_min: 0.1 # 최소값
    concentration_max: 10.0 # 최대값
    epsilon: 1e-6 # 수치 안정성

# T-Cell
tcell:
  contamination: 0.1
  n_estimators: 100
  window_size: 100

# Memory Cell
memory:
  capacity: 1000
  k_neighbors: 5

# 학습 설정
online_episodes: 200
skip_offline: false
early_stopping_patience: 20

# Ablation study
ablation:
  use_tcell: true
  use_memory: true
  use_xai: true
  offline_method: "iql"

# 모니터링 설정
monitoring:
  use_tensorboard: true
  use_wandb: false
  wandb_project: "finflow-rl"
  wandb_entity: null
  log_frequency: 10 # 로깅 주기 (스텝)

  # 안정성 체크
  stability_check:
    enabled: true
    q_value_threshold: 1000.0
    gradient_clip: 0.5  # 1.0 → 0.5 (더 안정적인 학습)
    check_frequency: 100

# 분포적 RL 설정 (TQC용)
distributional:
  n_quantiles: 32 # 분위수 개수
  kappa: 1.0 # Huber loss kappa
  risk_measure: "neutral" # neutral, cvar, wang
  risk_param: 0.1 # risk measure 매개변수
  quantile_embedding_dim: 64

# 기타
log_level: "INFO"
save_freq: 100 # 체크포인트 저장 빈도 (스텝 단위)

# 목적함수 설정 (PortfolioObjective)
objectives:
  # Differential Sharpe
  sharpe_beta: 1.0 # Sharpe 보너스 가중치
  sharpe_ema_alpha: 0.99 # EMA 평활 계수
  sharpe_epsilon: 1.0e-8

  # CVaR 제약
  cvar_alpha: 0.05 # 5% CVaR
  cvar_target: -0.02 # 목표 CVaR (최대 -2% 손실)
  lambda_cvar: 1.0 # CVaR 페널티 가중치

  # 추가 페널티
  lambda_turn: 0.1 # 턴오버 페널티
  lambda_dd: 0.0 # 드로다운 페널티

  # 보상 정규화
  r_clip: 5.0 # 보상 클리핑
  reward_ema_alpha: 0.99 # 보상 EMA

# 목표 메트릭
targets:
  sharpe_ratio: 1.5
  max_drawdown: 0.25
  cvar_95: -0.02

# 베이스라인 설정
baselines:
  equal_weight:
    enabled: true
  standard_sac:
    enabled: true
    n_episodes: 100
    hidden_dims: [256, 256]
